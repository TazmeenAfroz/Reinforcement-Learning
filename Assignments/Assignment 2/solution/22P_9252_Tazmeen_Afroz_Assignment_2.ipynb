{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Consider the current parking system of a city which charges a fixed rate for parking. The city is struggling to keep up with the increased demand. To address this issue, the city council has decided to modify the pricing scheme to better promote social welfare. In general, the city considers social welfare higher when more parking is being used, the exception being that the city prefers that at least one spot is left unoccupied (so that it is available in case someone really needs it). The city council has created a Markov decision process (MDP) to model the demand for parking with a reward function that reflects its preferences. Now the city has hired you — an expert in dynamic programming — to help determine an optimal policy.\n",
        "\n",
        "The first cell contains that defines the environment as ParkingLot class. The transition function has already been defined that gives the probabilities of next state and reward given a certain state and action.\n",
        "\n",
        "**Do not change the contents of the following cell.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "22m16d13GVqn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7zTbiaZYORxK"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "class ParkingLot:\n",
        "    def __init__(self, S, A):\n",
        "        \"\"\"\n",
        "        Initialize the ParkingLot environment.\n",
        "        :param S: Number of states (parking spots occupied)\n",
        "        :param A: Number of actions (pricing levels)\n",
        "        \"\"\"\n",
        "        self.S = S  # Number of states\n",
        "        self.A = A  # Number of actions\n",
        "\n",
        "    def transitions(self, state, action):\n",
        "        \"\"\"\n",
        "        Compute the transition probabilities and rewards for a given state and action.\n",
        "        :param state: The current state\n",
        "        :param action: The chosen action\n",
        "        :return: 2D NumPy array where each row index is next_state,\n",
        "                 the first column is reward, and the second column is transition probability\n",
        "        \"\"\"\n",
        "        # Define reward function\n",
        "        reward = 5 if state == self.S - 1 else (10 if state == self.S - 2 else (-5 if state == 0 else 0))\n",
        "\n",
        "        # Define transition probabilities based on action (pricing level)\n",
        "        probabilities = np.zeros(self.S)\n",
        "\n",
        "        # Dynamically adjust probability based on action scaling\n",
        "        demand_increase_prob = max(0.7 - (0.1 * action), 0.1)  # Higher pricing reduces demand increase\n",
        "        demand_decrease_prob = min(0.1 * action, 0.6)  # Higher pricing increases departure\n",
        "        stay_prob = 1.0 - demand_increase_prob - demand_decrease_prob\n",
        "\n",
        "        if state < self.S - 1:\n",
        "            probabilities[state + 1] = demand_increase_prob  # Probability of increased occupancy\n",
        "        if state > 0:\n",
        "            probabilities[state - 1] = demand_decrease_prob  # Probability of decreased occupancy\n",
        "        probabilities[state] = stay_prob  # Probability of no change\n",
        "\n",
        "        # Create a 2D NumPy array with rewards and probabilities\n",
        "        transition_matrix = np.zeros((self.S, 2))  # First column: reward, Second column: probability\n",
        "\n",
        "        for next_state in range(self.S):\n",
        "            if probabilities[next_state] > 0:\n",
        "                transition_matrix[next_state] = [reward, probabilities[next_state]]\n",
        "\n",
        "        return transition_matrix\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For now, let's consider an environment with three parking spaces and three price points. Note that an environment with three parking spaces actually has four states — zero, one, two, or three spaces could be occupied. Initially the policy is equiprobable policy assignig equal probability to each action. This environment if only for showing you how to iterate over different values of policies and transition probabilities.\n",
        "\n",
        "You can change the number of spaces and actions to see different transitions."
      ],
      "metadata": {
        "id": "jCoD88jNKZfY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNTs-jhpQ89u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "num_spaces = 4\n",
        "num_actions = 3\n",
        "env = ParkingLot(num_spaces,num_actions)\n",
        "\n",
        "V=np.zeros(env.S)\n",
        "\n",
        "\n",
        "#random policy\n",
        "pi = np.full((num_spaces, num_actions), 1.0 / num_actions)  # Uniform probabilities\n",
        "\n",
        "\n",
        "# for action,action_prob in enumerate(pi[2]):\n",
        "#   print(f\"Action: {action}: Action Probability: {action_prob}\")\n",
        "\n",
        "\n",
        "for state,action_probs in enumerate(pi):\n",
        "  print(f\"State: {state}: Action Probabilities: {action_probs}\")\n",
        "\n",
        "\n",
        "# Test a specific transition\n",
        "state, action = 2, 1\n",
        "transition_result = env.transitions(state, action)\n",
        "\n",
        "for new_state, (reward, probability) in enumerate(transition_result):\n",
        "    print(f\"Next State: {new_state}, Reward: {reward}, Probability: {probability}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9Ha7V93UKZI",
        "outputId": "e24852ae-795e-4a84-d6de-10449417c7a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: 0: Action Probabilities: [0.33333333 0.33333333 0.33333333]\n",
            "State: 1: Action Probabilities: [0.33333333 0.33333333 0.33333333]\n",
            "State: 2: Action Probabilities: [0.33333333 0.33333333 0.33333333]\n",
            "State: 3: Action Probabilities: [0.33333333 0.33333333 0.33333333]\n",
            "Next State: 0, Reward: 0.0, Probability: 0.0\n",
            "Next State: 1, Reward: 10.0, Probability: 0.1\n",
            "Next State: 2, Reward: 10.0, Probability: 0.30000000000000004\n",
            "Next State: 3, Reward: 10.0, Probability: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cells contain empty definitions of different fucntions that you need to fill in in order to find optimal policy using policy iteration and value iteration. Please make sure that the code must work for any number of states and actions. env.S gives you the number of states and env.A gives you the number of actions defined for a particular environment."
      ],
      "metadata": {
        "id": "OY69dULcK3iX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy(V,pi,env,gamma,theta):\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(env.S):\n",
        "            v = V[s]\n",
        "            v_new = 0\n",
        "            for a in range(env.A):\n",
        "                for s_next in range(env.S):\n",
        "                    transition = env.transitions(s, a)[s_next]\n",
        "                    r, prob = transition[0], transition[1]\n",
        "                    v_new += pi[s][a] * prob * (r + gamma * V[s_next])\n",
        "\n",
        "            V[s] = v_new\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n"
      ],
      "metadata": {
        "id": "sUxVUBArPGvw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def improve_policy(V,pi,env,gamma):\n",
        "  policy_stable = True\n",
        "  for s in range(env.S):\n",
        "        old_action = np.copy(pi[s])\n",
        "\n",
        "\n",
        "        q_values = np.zeros(env.A)\n",
        "        for a in range(env.A):\n",
        "            for s_next in range(env.S):\n",
        "                transition = env.transitions(s, a)[s_next]\n",
        "                r, prob = transition[0], transition[1]\n",
        "                q_values[a] += prob * (r + gamma * V[s_next])\n",
        "\n",
        "\n",
        "        best_action = np.argmax(q_values)\n",
        "\n",
        "\n",
        "        pi[s] = np.zeros(env.A)\n",
        "        pi[s][best_action] = 1.0  # Setting the probability of the best action to 1 so it gets chosen\n",
        "\n",
        "        if not np.array_equal(pi[s], old_action):\n",
        "            policy_stable = False\n",
        "  return policy_stable"
      ],
      "metadata": {
        "id": "KiDkdaB8RnMz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(V,pi,env,gamma,theta):\n",
        "    while True:\n",
        "        V = evaluate_policy(V, pi, env, gamma, theta)\n",
        "        policy_stable = improve_policy(V, pi, env, gamma)\n",
        "\n",
        "        if policy_stable:\n",
        "            break\n",
        "\n",
        "    return V,pi"
      ],
      "metadata": {
        "id": "XrUpio4rR4xV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(V, pi, env, gamma, theta):\n",
        "    while True:\n",
        "        delta = 0\n",
        "\n",
        "        best_actions = np.zeros(env.S, dtype=int)\n",
        "\n",
        "        for s in range(env.S):\n",
        "            v = V[s]\n",
        "\n",
        "            q_values = np.zeros(env.A)\n",
        "            for a in range(env.A):\n",
        "                for s_next in range(env.S):\n",
        "                    transition = env.transitions(s, a)[s_next]\n",
        "                    r, prob = transition[0], transition[1]\n",
        "                    q_values[a] += prob * (r + gamma * V[s_next])\n",
        "\n",
        "\n",
        "\n",
        "            V[s] = np.max(q_values)\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "            best_actions[s] = np.argmax(q_values)\n",
        "        if delta < theta:\n",
        "\n",
        "            for s in range(env.S):\n",
        "                pi[s] = np.zeros(env.A)\n",
        "                pi[s][best_actions[s]] = 1.0\n",
        "            break\n",
        "\n",
        "    return V, pi"
      ],
      "metadata": {
        "id": "Vq381jgQR9UE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell contains a new environment with different number of spaces and actions. This cell tests your implemented functions.\n",
        "\n",
        "**Do not change the contents of the following cell.**"
      ],
      "metadata": {
        "id": "gvhII8IYLI0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "num_spaces = 6\n",
        "num_actions = 3\n",
        "env = ParkingLot(num_spaces,num_actions)\n",
        "gamma = 0.9\n",
        "theta = 0.1\n",
        "\n",
        "\n",
        "V=np.zeros(env.S)\n",
        "\n",
        "\n",
        "#random policy\n",
        "pi = np.full((num_spaces, num_actions), 1.0 / num_actions)  # Uniform probabilities\n",
        "\n",
        "\n",
        "\n",
        "V = evaluate_policy(V,pi,env,gamma,theta)\n",
        "\n",
        "V_actual = np.array([0.98774032,9.73784006,13.03387846,16.00978726,19.47441585 ,5.14059646])\n",
        "\n",
        "assert np.isclose(V, V_actual, atol=0.01).all(), f\"Values are not within the first two decimal places: {V} vs {V_actual}\"\n",
        "\n",
        "V,pi = policy_iteration(V,pi,env,gamma,theta)\n",
        "pi_actual=np.array([[1., 0., 0.],\n",
        "       [1., 0., 0.],\n",
        "       [1., 0., 0.],\n",
        "       [1., 0., 0.],\n",
        "       [0., 0., 1.],\n",
        "       [0., 0., 1.]])\n",
        "assert (pi==pi_actual).all(), f\"Incorrect policy\"\n",
        "print(\"All tests passed!\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1903a0f-c503-4e9f-bcaf-2beedbc0fdcd",
        "id": "4KuKg-MNI6T3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell contains a new environment with different number of spaces and actions. This cell tests your implemented functions.\n",
        "\n",
        "**Do not change the contents of the following cell.**"
      ],
      "metadata": {
        "id": "l_4QMmyBLajz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_5-Ruebma60o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "num_spaces = 10\n",
        "num_actions = 3\n",
        "env = ParkingLot(num_spaces,num_actions)\n",
        "gamma = 0.9\n",
        "theta = 0.1\n",
        "\n",
        "\n",
        "V=np.zeros(env.S)\n",
        "\n",
        "\n",
        "#random policy\n",
        "pi = np.full((num_spaces, num_actions), 1.0 / num_actions)  # Uniform probabilities\n",
        "\n",
        "\n",
        "\n",
        "V = evaluate_policy(V,pi,env,gamma,theta)\n",
        "\n",
        "\n",
        "\n",
        "V_actual = np.array([ -3.39512584,  3.85951704,  5.8585558 ,  7.31664517,  8.93320282, 10.86460577, 13.20128222, 16.03631496, 19.47875764,  5.14121156])\n",
        "\n",
        "assert np.isclose(V, V_actual, atol=0.01).all(), f\"Values are not within the first two decimal places: {V} vs {V_actual}\"\n",
        "\n",
        "V,pi = policy_iteration(V,pi,env,gamma,theta)\n",
        "pi_actual=np.array([[1., 0., 0.],\n",
        "       [1., 0., 0.],\n",
        "       [1., 0., 0.],\n",
        "       [1., 0., 0.],\n",
        "       [1., 0., 0.],\n",
        "       [1., 0., 0.],\n",
        "       [1., 0., 0.],\n",
        "       [1., 0., 0.],\n",
        "       [0., 0., 1.],\n",
        "       [0., 0., 1.]])\n",
        "assert (pi==pi_actual).all(), f\"Incorrect policy\"\n",
        "print(\"All tests passed!\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "396205f5-a3f6-483a-b1ea-6b6a642d4f3f",
        "id": "Cn6B5t99a9iL"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus point**\n",
        "Create a new environment in the following cell with any number of states and actions and test the implementation of your value iteration algorithm in this cell."
      ],
      "metadata": {
        "id": "wRu8sgwYMNha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_spaces = 9\n",
        "num_actions = 3\n",
        "env = ParkingLot(num_spaces, num_actions)\n",
        "gamma = 0.7\n",
        "theta = 0.01\n",
        "\n",
        "V = np.zeros(env.S)\n",
        "pi = np.full((num_spaces, num_actions), 1.0 / num_actions)\n",
        "\n",
        "V, pi = value_iteration(V, pi, env, gamma, theta)\n",
        "\n",
        "print(\"Optimal Value Function (V):\", V)\n",
        "print(\"Optimal Policy (pi):\\n\", pi)\n"
      ],
      "metadata": {
        "id": "2SawIW0UMpcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b8f77b2-cec7-4ced-a44e-bbc88256d3db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Value Function (V): [-4.63462073  0.98029424  1.58738691  2.56271348  4.13337024  6.66477439\n",
            " 10.74560493 17.32471158  6.23475288]\n",
            "Optimal Policy (pi):\n",
            " [[0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UaSXrKUsmKHT"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}