{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Consider the current parking system of a city which charges a fixed rate for parking. The city is struggling to keep up with the increased demand. To address this issue, the city council has decided to modify the pricing scheme to better promote social welfare. In general, the city considers social welfare higher when more parking is being used, the exception being that the city prefers that at least one spot is left unoccupied (so that it is available in case someone really needs it). The city council has created a Markov decision process (MDP) to model the demand for parking with a reward function that reflects its preferences. Now the city has hired you — an expert in dynamic programming — to help determine an optimal policy.\n","\n","The first cell contains that defines the environment as ParkingLot class. The transition function has already been defined that gives the probabilities of next state and reward given a certain state and action.\n","\n","**Do not change the contents of the following cell.**\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"22m16d13GVqn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7zTbiaZYORxK"},"outputs":[],"source":["\n","import numpy as np\n","\n","class ParkingLot:\n","    def __init__(self, S, A):\n","        \"\"\"\n","        Initialize the ParkingLot environment.\n","        :param S: Number of states (parking spots occupied)\n","        :param A: Number of actions (pricing levels)\n","        \"\"\"\n","        self.S = S  # Number of states\n","        self.A = A  # Number of actions\n","\n","    def transitions(self, state, action):\n","        \"\"\"\n","        Compute the transition probabilities and rewards for a given state and action.\n","        :param state: The current state\n","        :param action: The chosen action\n","        :return: 2D NumPy array where each row index is next_state,\n","                 the first column is reward, and the second column is transition probability\n","        \"\"\"\n","        # Define reward function\n","        reward = 5 if state == self.S - 1 else (10 if state == self.S - 2 else (-5 if state == 0 else 0))\n","\n","        # Define transition probabilities based on action (pricing level)\n","        probabilities = np.zeros(self.S)\n","\n","        # Dynamically adjust probability based on action scaling\n","        demand_increase_prob = max(0.7 - (0.1 * action), 0.1)  # Higher pricing reduces demand increase\n","        demand_decrease_prob = min(0.1 * action, 0.6)  # Higher pricing increases departure\n","        stay_prob = 1.0 - demand_increase_prob - demand_decrease_prob\n","\n","        if state < self.S - 1:\n","            probabilities[state + 1] = demand_increase_prob  # Probability of increased occupancy\n","        if state > 0:\n","            probabilities[state - 1] = demand_decrease_prob  # Probability of decreased occupancy\n","        probabilities[state] = stay_prob  # Probability of no change\n","\n","        # Create a 2D NumPy array with rewards and probabilities\n","        transition_matrix = np.zeros((self.S, 2))  # First column: reward, Second column: probability\n","\n","        for next_state in range(self.S):\n","            if probabilities[next_state] > 0:\n","                transition_matrix[next_state] = [reward, probabilities[next_state]]\n","\n","        return transition_matrix\n","\n","\n"]},{"cell_type":"markdown","source":["For now, let's consider an environment with three parking spaces and three price points. Note that an environment with three parking spaces actually has four states — zero, one, two, or three spaces could be occupied. Initially the policy is equiprobable policy assignig equal probability to each action. This environment if only for showing you how to iterate over different values of policies and transition probabilities.\n","\n","You can change the number of spaces and actions to see different transitions."],"metadata":{"id":"jCoD88jNKZfY"}},{"cell_type":"code","source":[],"metadata":{"id":"SNTs-jhpQ89u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example usage\n","num_spaces = 4\n","num_actions = 3\n","env = ParkingLot(num_spaces,num_actions)\n","\n","V=np.zeros(env.S)\n","\n","\n","#random policy\n","pi = np.full((num_spaces, num_actions), 1.0 / num_actions)  # Uniform probabilities\n","\n","\n","# for action,action_prob in enumerate(pi[2]):\n","#   print(f\"Action: {action}: Action Probability: {action_prob}\")\n","\n","\n","for state,action_probs in enumerate(pi):\n","  print(f\"State: {state}: Action Probabilities: {action_probs}\")\n","\n","\n","# Test a specific transition\n","state, action = 2, 1\n","transition_result = env.transitions(state, action)\n","\n","for new_state, (reward, probability) in enumerate(transition_result):\n","    print(f\"Next State: {new_state}, Reward: {reward}, Probability: {probability}\")\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q9Ha7V93UKZI","outputId":"34a4974b-cf77-4aeb-ca13-b20bb2689cc8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["State: 0: Action Probabilities: [0.33333333 0.33333333 0.33333333]\n","State: 1: Action Probabilities: [0.33333333 0.33333333 0.33333333]\n","State: 2: Action Probabilities: [0.33333333 0.33333333 0.33333333]\n","State: 3: Action Probabilities: [0.33333333 0.33333333 0.33333333]\n","State: 4: Action Probabilities: [0.33333333 0.33333333 0.33333333]\n","State: 5: Action Probabilities: [0.33333333 0.33333333 0.33333333]\n","State: 6: Action Probabilities: [0.33333333 0.33333333 0.33333333]\n","State: 7: Action Probabilities: [0.33333333 0.33333333 0.33333333]\n","State: 8: Action Probabilities: [0.33333333 0.33333333 0.33333333]\n","State: 9: Action Probabilities: [0.33333333 0.33333333 0.33333333]\n","Next State: 0, Reward: 0.0, Probability: 0.0\n","Next State: 1, Reward: 0.0, Probability: 0.1\n","Next State: 2, Reward: 0.0, Probability: 0.30000000000000004\n","Next State: 3, Reward: 0.0, Probability: 0.6\n","Next State: 4, Reward: 0.0, Probability: 0.0\n","Next State: 5, Reward: 0.0, Probability: 0.0\n","Next State: 6, Reward: 0.0, Probability: 0.0\n","Next State: 7, Reward: 0.0, Probability: 0.0\n","Next State: 8, Reward: 0.0, Probability: 0.0\n","Next State: 9, Reward: 0.0, Probability: 0.0\n"]}]},{"cell_type":"markdown","source":["The following cells contain empty definitions of different fucntions that you need to fill in in order to find optimal policy using policy iteration and value iteration. Please make sure that the code must work for any number of states and actions. env.S gives you the number of states and env.A gives you the number of actions defined for a particular environment."],"metadata":{"id":"OY69dULcK3iX"}},{"cell_type":"code","source":["def evaluate_policy(V,pi,env,gamma,theta):\n","  return V\n"],"metadata":{"id":"sUxVUBArPGvw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def improve_policy(V,pi,env,gamma):\n","  policy_stable = True\n","  return policy_stable"],"metadata":{"id":"KiDkdaB8RnMz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def policy_iteration(V,pi,env,gamma,theta):\n","  return V,pi"],"metadata":{"id":"XrUpio4rR4xV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def value_iteration(V,pi,env,gamma,theta):\n","  return V,pi"],"metadata":{"id":"Vq381jgQR9UE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following cell contains a new environment with different number of spaces and actions. This cell tests your implemented functions.\n","\n","**Do not change the contents of the following cell.**"],"metadata":{"id":"gvhII8IYLI0D"}},{"cell_type":"code","source":["# Example usage\n","num_spaces = 6\n","num_actions = 3\n","env = ParkingLot(num_spaces,num_actions)\n","gamma = 0.9\n","theta = 0.1\n","\n","\n","V=np.zeros(env.S)\n","\n","\n","#random policy\n","pi = np.full((num_spaces, num_actions), 1.0 / num_actions)  # Uniform probabilities\n","\n","\n","\n","V = evaluate_policy(V,pi,env,gamma,theta)\n","\n","V_actual = np.array([0.98774032,9.73784006,13.03387846,16.00978726,19.47441585 ,5.14059646])\n","\n","assert np.isclose(V, V_actual, atol=0.01).all(), f\"Values are not within the first two decimal places: {V} vs {V_actual}\"\n","\n","V,pi = policy_iteration(V,pi,env,gamma,theta)\n","pi_actual=np.array([[1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [0., 0., 1.],\n","       [0., 0., 1.]])\n","assert (pi==pi_actual).all(), f\"Incorrect policy\"\n","print(\"All tests passed!\")\n","\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"outputId":"7c028c30-01bc-429c-b8f2-647338f67150","id":"4KuKg-MNI6T3"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"Incorrect policy","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-03215757a02c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m        \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m        [0., 0., 1.]])\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mpi_actual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Incorrect policy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All tests passed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: Incorrect policy"]}]},{"cell_type":"markdown","source":["The following cell contains a new environment with different number of spaces and actions. This cell tests your implemented functions.\n","\n","**Do not change the contents of the following cell.**"],"metadata":{"id":"l_4QMmyBLajz"}},{"cell_type":"code","source":[],"metadata":{"id":"_5-Ruebma60o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example usage\n","num_spaces = 10\n","num_actions = 3\n","env = ParkingLot(num_spaces,num_actions)\n","gamma = 0.9\n","theta = 0.1\n","\n","\n","V=np.zeros(env.S)\n","\n","\n","#random policy\n","pi = np.full((num_spaces, num_actions), 1.0 / num_actions)  # Uniform probabilities\n","\n","\n","\n","V = evaluate_policy(V,pi,env,gamma,theta)\n","\n","\n","\n","V_actual = np.array([ -3.39512584,  3.85951704,  5.8585558 ,  7.31664517,  8.93320282, 10.86460577, 13.20128222, 16.03631496, 19.47875764,  5.14121156])\n","\n","assert np.isclose(V, V_actual, atol=0.01).all(), f\"Values are not within the first two decimal places: {V} vs {V_actual}\"\n","\n","V,pi = policy_iteration(V,pi,env,gamma,theta)\n","pi_actual=np.array([[1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [1., 0., 0.],\n","       [0., 0., 1.],\n","       [0., 0., 1.]])\n","assert (pi==pi_actual).all(), f\"Incorrect policy\"\n","print(\"All tests passed!\")\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"outputId":"f94423c6-028a-4406-fef7-958570195cf4","id":"Cn6B5t99a9iL"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"Values are not within the first two decimal places: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] vs [-3.39512584  3.85951704  5.8585558   7.31664517  8.93320282 10.86460577\n 13.20128222 16.03631496 19.47875764  5.14121156]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-4949cb85ec34>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mV_actual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m3.39512584\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m3.85951704\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m5.8585558\u001b[0m \u001b[0;34m,\u001b[0m  \u001b[0;36m7.31664517\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m8.93320282\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10.86460577\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13.20128222\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16.03631496\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19.47875764\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m5.14121156\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_actual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Values are not within the first two decimal places: {V} vs {V_actual}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: Values are not within the first two decimal places: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] vs [-3.39512584  3.85951704  5.8585558   7.31664517  8.93320282 10.86460577\n 13.20128222 16.03631496 19.47875764  5.14121156]"]}]},{"cell_type":"markdown","source":["**Bonus point**\n","Create a new environment in the following cell with any number of states and actions and test the implementation of your value iteration algorithm in this cell."],"metadata":{"id":"wRu8sgwYMNha"}},{"cell_type":"code","source":[],"metadata":{"id":"2SawIW0UMpcU"},"execution_count":null,"outputs":[]}]}